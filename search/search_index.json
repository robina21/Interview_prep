{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>\ud83d\udc4b Welcome! Hi there! I\u2019m glad you stopped by. This site is your go-to resource to prepare for Data Engineering interviews\u2014from SQL and Python to cloud tools and real-world scenarios.</p> <p>I\u2019m a Data Engineer with 6+ years of experience, and I\u2019ve built and scaled data platforms across industries. Everything here is crafted from hands-on experience and countless interviews\u2014both as a candidate and an interviewer.</p> <p>Let\u2019s get you interview-ready. \ud83d\ude80</p>"},{"location":"actual_interview_questions/","title":"Real World Interview Questions","text":"<ol> <li>Given a 5gb file how will you read in python and load to a DB</li> <li>How will you bring 1000 files from oracle and what are the steps you will take to load and build a data pipeline either in aws or azure</li> <li>what kind of data quality checks have you worked on?</li> <li>What transformations have you done ?</li> <li>How do you remove duplicates in pyspark [what other methods other than drop.duplicates do you know ?]</li> <li>How do you remove nulls in pyspark ?</li> <li>if you get number as data type from s3 and athena doesnt have that data type how will you handle that data type mismacth ? </li> <li>How do you update a col with another value in pyspark ?</li> <li>How do you read excel file in pyspark (pyspark.read.csv()</li> <li>What do you do as a data engineer ?</li> </ol> <p>Cognizant Interview: Diff between all the joins left, right, inner, fullouterjoin, leftouter/rightouter joins nvl,nvl2, lag, lead  diff types of windown fun  rank and dense rank given 2 tables give me the result kind of questions diff between cursor &amp; triger give me a situation and query for CTE what func do you use if else for in sql, I said case statements  Update salary by 5% for all employees in department C what do you know about index  emplo and dep table only 2 rows in both employ id and dep id and dept has dept it rows are 1a , 1, 2a, 2, 3a,, 3 and dept tab has 1,2,34,5 out dept naame is a,b,c,d,e empt  what is view ? does it get refreshed when the physical table no.of rows changes ? what is merge ? query for second highest salary ? what is row_id concept ? delete duplicate id sql  what are the diff transformations you have done in Informatica </p>"},{"location":"etl/","title":"ETL","text":"<p>How do you optimize ETL performance? </p> <p>Parallel processing breaking down ETL tasks into smaller units that can be executed concurrently across multiple threads, processors, or nodes</p> <p>Data partitioning By dividing large datasets into smaller, manageable partitions based on predefined criteria</p> <p>Optimizing SQL queries  The SQL queries used in ETL processes can be optimized to improve performance by reducing execution time and resource consumption. </p> <p>Incremental loading and CDC (change data capture)</p> <p>Incremental loading involves updating only the changed or new data since the last ETL run rather than processing the entire dataset.</p> <p>Describe the use of Lookup Transformation in ETL.</p> <p>How do you write efficient SQL queries for ETL? Indexing Ensure that primary and foreign key columns are indexed to speed up joins and lookups.</p> <p>Optimizing joins is another strategy</p> <p>Pitfalls to avoid There are also common pitfalls that hamper the performance of SQL queries. These include:</p> <p>SELECT *: Do not select all columns when necessary. It is better to specify the required columns to reduce the amount of data processed and transferred. Performing many functions in WHERE clauses: It\u2019s better to calculate values outside the query or use indexed computed columns. Not using batch processing: Break down large operations into smaller batches to avoid long-running transactions and reduce lock contention. Inappropriate data types: Choose the most efficient data types for your columns to save storage and improve performance.</p> <p>How to optimise a sql query ? avoid using select * statements,  Use the right WHERE clause (filter early) Write efficient JOINs \u201cIndexes help the optimizer avoid full table scans.\u201d Use INNER JOIN instead of LEFT JOIN if possible     &gt; WHY it helps</p> <pre><code>LEFT JOIN keeps unmatched rows \u2192 more data processed\n\nINNER JOIN filters earlier\n\nFewer rows \u2192 faster join\n</code></pre> <p>Use EXISTS instead of IN (for large datasets)</p> <p>First, I analyze the query to understand which part is slow. I avoid using SELECT * and fetch only required columns to reduce data scanning.</p> <p>I check joins and prefer INNER JOINs instead of LEFT JOINs wherever possible, because inner joins reduce the number of rows processed.</p> <p>I make sure indexes exist on columns used in WHERE clauses and JOIN conditions, and I avoid applying functions on indexed columns so the index can be used effectively.</p> <p>I also check the execution plan to see if there are any full table scans and optimize accordingly.\u201d</p> <p>why should you not index all columns  We don\u2019t index all columns because indexes take additional storage and every insert, update, or delete operation also needs to update the index. Too many indexes slow down write performance and increase maintenance overhead. So we only index columns that are frequently used in joins, filters, and sorting.\u201d</p> <p>what are the different transformations have you done ? Trimming spaces, converting text to upper/lower case</p> <p>Handling missing values using defaults or conditional logic</p> <p>Filtering out invalid or rejected records</p> <p>Data type conversions String to ---&gt; date / timestamp conversions \u201cI used CAST, CONVERT, CASE, and TRY_CAST to avoid runtime failures.\u201d \u201cI first check the format of the incoming string to ensure it matches the expected date format. For example, DD-MM-YYYY or YYYY-MM-DD.\u201d</p> <p>TO_DATE(source_date, 'DD-MM-YYYY') in python/pandas df['target_date'] = pd.to_datetime(df['source_date'], format='%d-%m-%Y', errors='coerce')</p> <p>* ANSWER THE BELOW ** \u201cIf the source column is decimal but Athena expects an integer, I first check if it\u2019s possible to safely cast or convert the type. If yes, I perform the conversion in Pandas or PySpark before loading, ensuring no data is lost. If casting isn\u2019t feasible and the business allows, I load it as a string to avoid load errors. Throughout, I validate the data to make sure the transformation aligns with business requirements.\u201d</p> <p>joins &amp; lookups I performed lookups and joins across multiple source systems</p> <p>Current project</p> <p>I bring the data from s3 for example and do a virus scan [affected files)] since I work with healt care data we use a aws service called macie which will check for PHI/PII information and flag it then we get the data dict and get the data types and create tables and then load to redshift </p> <p>then we do some dq checks  like checking for null vs empty consistency checks</p>"},{"location":"informatica/","title":"Informatica","text":"<p>what did you do in informatica ? \u201cIn Informatica, I worked on  maintaining ETL mappings and workflows. I used it to perform source-to-target data mapping, apply transformations like filtering, joins, lookups, aggregations, and data cleansing, and load data into target tables while ensuring data quality and consistency.\u201d</p> <p>\u201cI analyzed source data, designed mappings based on source-to-target documents, implemented transformations, handled slowly changing dimensions, and scheduled workflows to automate daily and incremental loads.\u201d</p> <p>What is the difference between Source Qualifier and Filter transformation? SQL is used to filter rows at the source level before they enter the mapping, reducing the volume of data processed, whereas a Filter transformation filters data later in the pipeline.</p> <p>What is Target Load Order and when is it used? It defines the order in which data is loaded into targets, particularly crucial when targets have primary-foreign key constraints. It is configured in the Session Properties.</p> <p>What are the different types of targets in Informatica? Relational tables (databases), Flat files (CSV, fixed-width), XML, and other applications.</p> <p>\ud83d\udd39 How Do You Do a Filter in Informatica? Primary way: Filter Transformation</p> <p>\u201cI use a Filter transformation to filter records based on a condition.\u201d</p> <p>Example:</p> <p>Condition:</p> <p>salary &gt; 50000 AND status = 'ACTIVE'</p> <p>Only records meeting the condition pass through.</p> <p>\ud83d\udd39 Other Ways to Filter in Informatica (Important) 1\ufe0f\u20e3 Source Qualifier Filter (Best for performance)</p> <p>\u201cI apply filters in the Source Qualifier whenever possible to push filtering to the source database.\u201d</p> <p>Example:</p> <p>WHERE created_date &gt;= SYSDATE - 1</p> <p>\u2714 Reduces data volume early \u2714 Improves performance</p> <p>2\ufe0f\u20e3 Expression Transformation</p> <p>\u201cI use Expression transformation with conditional logic and route records using ports.\u201d</p> <p>Example logic:</p> <p>IF(status = 'ACTIVE', 1, 0)</p> <p>Then connect only valid records.</p> <p>3\ufe0f\u20e3 Router Transformation</p> <p>\u201cRouter transformation is used when I need multiple filtering conditions in one place.\u201d</p> <p>Example:</p> <p>Group 1: status = 'ACTIVE'</p> <p>Group 2: status = 'INACTIVE'</p>"},{"location":"pandas_interview_questions/","title":"Pandas Interview Questions for Data Engineers (with Explanations)","text":""},{"location":"pandas_interview_questions/#data-analysis-pandas-numpy","title":"\ud83d\udcca Data Analysis (Pandas / NumPy)","text":"<p>Q13. How do you handle missing data in pandas?</p> <pre><code>df.dropna(), df.fillna(0)\n</code></pre> <p>Q14. How to group data in pandas?</p> <pre><code>df.groupby('column').agg({'sales': 'sum'})\n</code></pre> <p>Q15. How to filter rows in a DataFrame?</p> <pre><code>df[df['value'] &gt; 100]\n</code></pre>"},{"location":"pandas_interview_questions/#data-loading-exploration","title":"\ud83d\udce5 Data Loading &amp; Exploration","text":"<p>Q1. How do you read a large CSV file in chunks using Pandas? Use <code>chunksize</code> to load the file in smaller parts to save memory.</p> <pre><code>chunks = pd.read_csv('data.csv', chunksize=10000)\nfor chunk in chunks:\n    process(chunk)\n</code></pre> <p>Q2. How do you get a quick summary of a DataFrame? Use <code>info()</code>, <code>describe()</code>, and <code>head()</code> to get structure, statistics, and sample data.</p> <pre><code>df.info()\ndf.describe()\ndf.head()\n</code></pre> <p>Q3. How do you detect missing values? Use <code>isnull().sum()</code> to count nulls per column.</p> <pre><code>df.isnull().sum()\n</code></pre> <p>Difference between loc and iloc?</p> <p>Answer: loc is label-based indexing. iloc is position-based indexing.</p>"},{"location":"pandas_interview_questions/#data-cleaning","title":"\ud83e\uddf9 Data Cleaning","text":"<p>Q4. How to drop rows with any null values? <code>dropna()</code> removes all rows with at least one NaN.</p> <pre><code>df.dropna()\n</code></pre> <p>Q5. How to fill missing values with the median of a column? <code>fillna()</code> can fill NaNs with median, mean, or custom values.</p> <pre><code>df['column'].fillna(df['column'].median(), inplace=True)\n</code></pre> <p>Q6. How to convert a column to datetime? Use <code>pd.to_datetime()</code> to ensure proper datetime parsing.</p> <pre><code>df['date'] = pd.to_datetime(df['date'])\n</code></pre> <p>Q7. How to remove duplicates from a DataFrame? <code>drop_duplicates()</code> keeps the first instance and drops repeated rows.</p> <pre><code>df.drop_duplicates(inplace=True)\n</code></pre>"},{"location":"pandas_interview_questions/#filtering-sorting-mapping","title":"\ud83d\udd0d Filtering, Sorting &amp; Mapping","text":"<p>Q8. How to filter rows where salary &gt; 100K and department = 'IT'? Use boolean conditions with <code>&amp;</code> for AND logic.</p> <pre><code>df[(df['salary'] &gt; 100000) &amp; (df['department'] == 'IT')]\n</code></pre> <p>Q9. How to apply a function to a column? Use <code>apply()</code> to perform row-wise or column-wise transformations.</p> <pre><code>df['salary_bracket'] = df['salary'].apply(lambda x: 'High' if x &gt; 100000 else 'Low')\n</code></pre> <p>Q10. How to sort data by two columns? Use <code>sort_values()</code> with <code>ascending</code> flags.</p> <pre><code>df.sort_values(by=['department', 'salary'], ascending=[True, False])\n</code></pre>"},{"location":"pandas_interview_questions/#groupby-aggregation","title":"\ud83d\udd01 GroupBy &amp; Aggregation","text":"<p>Q11. How to group by department and calculate average salary? <code>groupby()</code> with aggregation helps summarize data.</p> <pre><code>df.groupby('department')['salary'].mean()\n</code></pre> <p>Q12. How to get count and sum in one groupby operation? <code>agg()</code> allows multiple aggregations on grouped data.</p> <pre><code>df.groupby('department').agg({'salary': ['count', 'sum']})\n</code></pre>"},{"location":"pandas_interview_questions/#joins-merges","title":"\ud83d\udd17 Joins &amp; Merges","text":"<p>Q13. How to perform an inner join on two DataFrames? <code>pd.merge()</code> works like SQL JOINs.</p> <pre><code>pd.merge(df1, df2, on='id', how='inner')\n</code></pre> <p>Q14. How to do a left join using different column names? You can specify different key names using <code>left_on</code> and <code>right_on</code>.</p> <pre><code>pd.merge(df1, df2, left_on='emp_id', right_on='id', how='left')\n</code></pre>"},{"location":"pandas_interview_questions/#reshaping-data","title":"\ud83d\udccf Reshaping Data","text":"<p>Q15. How to pivot a table to summarize data? Use <code>pivot_table()</code> to create cross-tabulations.</p> <pre><code>df.pivot_table(index='department', columns='gender', values='salary', aggfunc='mean')\n</code></pre> <p>Q16. How to melt a wide table into a long format? <code>melt()</code> is useful for unpivoting columns into rows.</p> <pre><code>pd.melt(df, id_vars=['id'], var_name='attribute', value_name='value')\n</code></pre>"},{"location":"pandas_interview_questions/#optimization-performance","title":"\ud83e\udde0 Optimization &amp; Performance","text":"<p>Q17. How do you handle memory issues with large data sets? Use <code>dtype</code>, <code>usecols</code>, and <code>chunksize</code> to control memory usage.</p> <p>Q18. How to optimize string columns? Convert strings to category dtype for better performance.</p> <pre><code>df['category_column'] = df['category_column'].astype('category')\n</code></pre>"},{"location":"pandas_interview_questions/#date-time-operations","title":"\ud83d\udcc6 Date &amp; Time Operations","text":"<p>Q19. How to extract year and month from a datetime column? Use <code>dt</code> accessor to break down datetime parts.</p> <pre><code>df['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\n</code></pre> <p>Q20. How to calculate the number of days between two dates? Subtract datetime columns and use <code>.dt.days</code>.</p> <pre><code>df['days'] = (df['end_date'] - df['start_date']).dt.days\n</code></pre>"},{"location":"pandas_interview_questions/#real-world-case","title":"\ud83e\uddea Real-World Case","text":"<p>Q21. You get a daily CSV dump with millions of rows. How do you deduplicate and load only new records into a database? Use chunking, hash comparison, and staging tables to identify new data before loading.</p> <p>Q22. How to check for data consistency between two large DataFrames (e.g., source vs target)? Compare with <code>.equals()</code> or find differences using <code>concat()</code> and <code>drop_duplicates()</code>.</p> <pre><code>df1.equals(df2)\npd.concat([df1, df2]).drop_duplicates(keep=False)\n</code></pre> <p>How do you read large files in Pandas?</p> <p>Answer: Using chunk size to read data in smaller parts.</p>"},{"location":"pandas_interview_questions/#interview-tips","title":"\u2705 Interview Tips","text":"<ul> <li>Practice real-world data cleanup and profiling.</li> <li>Be fluent in joins, groupby, filtering, and performance tuning.</li> <li>Know how to work with large files and time-based operations.</li> </ul>"},{"location":"projects/","title":"Data Engineering Projects","text":""},{"location":"projects/#health-care-triangle-projects-roche","title":"Health Care Triangle Projects [ROCHE]","text":"<ul> <li>Developed Roche billing and cost optimization dashboards in Tableau for client reporting and decision-making.</li> <li>Extracted and modeled data from Amazon Redshift to support analytics and visualization.</li> <li>Designed and optimized SQL queries for accurate billing insights and trend analysis.</li> <li>Automated recurring billing reports from Redshift and delivered them to clients via scheduled emails.</li> <li>Improved reporting efficiency and reduced manual effort through automation.</li> </ul>"},{"location":"projects/#deloitte-projects","title":"Deloitte Projects","text":""},{"location":"projects/#project-1-oracle-azure-data-factory-snowflake-care-first","title":"Project 1 \u2014 Oracle \u2192 Azure Data Factory \u2192 Snowflake (Care First)","text":""},{"location":"projects/#objective","title":"Objective","text":"<p>Migrate historical data from Oracle to Snowflake using Azure Data Factory (ADF) with a SQL Server control layer for orchestration and tracking.</p>"},{"location":"projects/#what-i-built-and-implemented","title":"What I built and implemented","text":"<ul> <li>Designed an end\u2011to\u2011end migration framework for table\u2011by\u2011table loading.</li> <li>Created and maintained SQL Server stored procedures to control and track loads.</li> <li>Built metadata tables in SQL Server to manage:</li> <li>Source table list</li> <li>Load status</li> <li>Start/complete timestamps</li> <li>Error tracking</li> <li>Automated a table\u2011driven loop that:</li> <li>Reads the next table from metadata</li> <li>Updates status to \"STARTED\" in SQL Server</li> <li>Triggers Azure Data Factory pipeline</li> <li>Loads data into Snowflake</li> <li>Updates status to \"COMPLETED\" in SQL Server</li> <li>Moves to the next table</li> <li>Implemented restartability so failed tables could be rerun without reloading everything.</li> </ul>"},{"location":"projects/#tech-stack","title":"Tech stack","text":"<ul> <li>Oracle</li> <li>Azure Data Factory</li> <li>Snowflake</li> <li>SQL Server (control/metadata layer)</li> </ul>"},{"location":"projects/#project-2-oracle-aws-centene","title":"Project 2 \u2014 Oracle \u2192 AWS (Centene)","text":""},{"location":"projects/#objective_1","title":"Objective","text":"<p>Build an automated ingestion framework to move data from Oracle to AWS Redshift using AWS Glue and S3.</p>"},{"location":"projects/#what-i-built-and-implemented_1","title":"What I built and implemented","text":"<ul> <li>Developed AWS Glue jobs to:</li> <li>Create target tables in Redshift</li> <li>Load data from S3 into Redshift</li> <li>Designed a control file in S3 to drive loads (table name, file name, target, etc.).</li> <li>Built parameterized SQL templates for loading and validation.</li> <li>Updated DynamoDB configuration tables with metadata such as:</li> <li>File name</li> <li>Target table/location</li> <li>File size</li> <li>Load status</li> <li>Timestamps</li> <li>Implemented automated pipeline flow:</li> <li>Oracle extract \u2192 S3 landing</li> <li>Metadata written to DynamoDB</li> <li>Glue job reads config and loads Redshift</li> <li>Status updated on success/failure</li> <li>Performed Source\u2011to\u2011Target (S\u2192T) data mapping.</li> <li>Analyzed existing data flows in Informatica to align mappings with AWS implementation.</li> </ul>"},{"location":"projects/#tech-stack_1","title":"Tech stack","text":"<ul> <li>Oracle</li> <li>AWS S3</li> <li>AWS Glue</li> <li>Amazon Redshift</li> <li>DynamoDB</li> <li>Informatica (analysis)</li> </ul>"},{"location":"projects/#project-3-discovery-data-mapping-exploratory-project","title":"Project 3 \u2014 Discovery &amp; Data Mapping (Exploratory Project)","text":""},{"location":"projects/#objective_2","title":"Objective","text":"<p>Analyze source systems and define a clean, duplicate\u2011free loading strategy.</p>"},{"location":"projects/#what-i-did","title":"What I did","text":"<ul> <li>Performed deep data profiling and exploratory analysis.</li> <li>Created detailed source\u2011to\u2011target mappings.</li> <li>Wrote complex SQL queries to:</li> <li>Identify correct join paths across multiple tables</li> <li>Remove duplicates</li> <li>Apply necessary business filters</li> <li>Define primary keys and uniqueness rules</li> <li>Documented transformation logic for downstream ingestion.</li> </ul>"},{"location":"projects/#key-skills-demonstrated","title":"Key skills demonstrated","text":"<ul> <li>SQL joins (inner, left, multi\u2011table)</li> <li>Deduplication strategies</li> <li>Data quality validation</li> <li>Requirements gathering from raw data</li> </ul>"},{"location":"projects/#genentech-projects","title":"Genentech Projects","text":"<ul> <li>Designed and orchestrated end-to-end ETL workflows using AWS Step Functions to manage multi-step data pipelines.</li> <li>Developed and deployed Talend-based ETL pipelines for AWS cloud environments.</li> <li>Built and managed AWS Glue ETL jobs for data ingestion, transformation, and quality validation.</li> <li>Implemented custom data quality checks and schema evolution in AWS Glue to ensure reliable analytics.</li> <li>Designed automated sensitive data detection workflows using AWS Macie for data governance and compliance.</li> <li>Loaded and optimized datasets in Amazon Redshift Serverless for reporting and analytics.</li> <li>Developed complex Spark SQL and PySpark transformations for large-scale data processing.</li> <li>Migrated data from on-prem FTP servers to PostgreSQL as part of cloud modernization.</li> <li>Established reusable data quality frameworks for enterprise ETL pipelines.</li> </ul>"},{"location":"pyspark_interview_questions/","title":"PySpark Interview Questions &amp; Answers (With Definitions)","text":"<p>This page is formatted for MkDocs and can be saved as:</p> <pre><code>docs/pyspark-interview.md\n</code></pre>"},{"location":"pyspark_interview_questions/#1-what-is-pyspark-and-how-does-it-differ-from-regular-spark","title":"1) What is PySpark and how does it differ from regular Spark?","text":"<p>Definition: Apache Spark is a distributed computing engine for large-scale data processing. PySpark is the Python API for Apache Spark that allows you to write Spark applications using Python while Spark executes them on a distributed JVM-based engine.</p> <p>How it differs from regular (Scala) Spark: - Spark core is written in Scala/Java and runs on the JVM. - PySpark provides a Python interface, but computations are still executed by the Spark engine. - PySpark is easier for Python users, but may introduce slight serialization overhead compared to native Scala. - Performance is generally comparable when using DataFrames (because of Catalyst optimization).</p>"},{"location":"pyspark_interview_questions/#2-how-do-you-set-up-a-pyspark-environment","title":"2) How do you set up a PySpark environment?","text":"<p>Definition: A PySpark environment is a runtime setup where Python can communicate with a Spark cluster to execute distributed computations.</p> <p>Common ways to set it up:</p> <p>Local machine:</p> <pre><code>pip install pyspark\n</code></pre> <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"test\").getOrCreate()\n</code></pre> <p>Cloud/Production options: - AWS Glue \u2013 Managed Spark environment for ETL. - Databricks \u2013 Managed notebooks + clusters. - AWS EMR \u2013 You create and manage a Spark cluster.</p>"},{"location":"pyspark_interview_questions/#3-explain-the-architecture-of-apache-spark","title":"3) Explain the architecture of Apache Spark.","text":"<p>Definition: Spark uses a distributed master\u2013worker architecture to process data in parallel across a cluster.</p> <p>Components: - Driver: Controls the application, creates the DAG, and schedules tasks. - Cluster Manager: (YARN, Mesos, or Standalone) allocates resources. - Executors: Run tasks on worker nodes and store cached data. - Tasks: Smallest unit of work executed on partitions.</p> <p>Execution flow:</p> <pre><code>Driver \u2192 Logical Plan \u2192 Optimized Plan \u2192 DAG \u2192 Stages \u2192 Tasks \u2192 Executors\n</code></pre>"},{"location":"pyspark_interview_questions/#4-what-are-rdds-resilient-distributed-datasets","title":"4) What are RDDs (Resilient Distributed Datasets)?","text":"<p>Definition: An RDD is Spark\u2019s original distributed data structure representing an immutable, partitioned collection of elements that can be processed in parallel.</p> <p>Key properties: - Distributed across nodes - Immutable - Fault-tolerant via lineage - Lazy evaluated - Partitioned for parallelism</p> <p>Example:</p> <pre><code>rdd = spark.sparkContext.parallelize([1,2,3,4])\n</code></pre>"},{"location":"pyspark_interview_questions/#5-how-do-dataframes-and-datasets-differ-in-pyspark","title":"5) How do DataFrames and Datasets differ in PySpark?","text":"<p>Definition: - A DataFrame is a distributed table with named columns and a schema. - A Dataset is a strongly typed distributed collection (mainly used in Scala/Java).</p> Feature DataFrame Dataset Schema Yes Yes Type safety No (Python) Yes (Scala/Java) Performance Optimized Optimized Common in PySpark Yes Rare <p>In practice: Most PySpark work uses DataFrames.</p>"},{"location":"pyspark_interview_questions/#6-how-do-you-read-and-write-data-using-pyspark","title":"6) How do you read and write data using PySpark?","text":"<p>Definition: PySpark provides a unified data source API to read/write structured data from files, databases, and cloud storage.</p> <p>Read CSV:</p> <pre><code>df = spark.read.csv(\"s3://bucket/file.csv\", header=True, inferSchema=True)\n</code></pre> <p>Write Parquet:</p> <pre><code>df.write.mode(\"overwrite\").parquet(\"s3://bucket/output/\")\n</code></pre> <p>Read Parquet:</p> <pre><code>df = spark.read.parquet(\"s3://bucket/output/\")\n</code></pre>"},{"location":"pyspark_interview_questions/#7-what-are-common-transformations-and-actions","title":"7) What are common transformations and actions?","text":"<p>Definition: - Transformations create a new DataFrame/RDD but do not execute immediately (lazy). - Actions trigger execution of the Spark job.</p> <p>Common transformations: - <code>select()</code>, <code>filter()</code>, <code>withColumn()</code>, <code>groupBy()</code>, <code>join()</code></p> <p>Common actions: - <code>show()</code>, <code>count()</code>, <code>collect()</code>, <code>write()</code></p>"},{"location":"pyspark_interview_questions/#8-how-do-you-handle-missing-or-null-values","title":"8) How do you handle missing or null values?","text":"<p>Definition: Missing values (nulls) must be cleaned before analytics to avoid incorrect results.</p> <pre><code>df.dropna() drop duplicates\ndf.fillna(0) fill with value\ndf.fillna({\"age\": 30, \"salary\": 0})\n</code></pre>"},{"location":"pyspark_interview_questions/#9-explain-lazy-evaluation-in-pyspark","title":"9) Explain lazy evaluation in PySpark.","text":"<p>Definition: Lazy evaluation means Spark does not execute transformations immediately. Instead, it builds a logical plan (DAG) and waits for an action before running.</p> <p>Why this matters: - Allows Spark to optimize the full pipeline. - Reduces unnecessary computation.</p>"},{"location":"pyspark_interview_questions/#10-how-do-you-perform-joins-in-pyspark","title":"10) How do you perform joins in PySpark?","text":"<pre><code>df1.join(df2, on=\"id\", how=\"inner\")\n</code></pre> <p>Join types: - <code>inner</code>, <code>left</code>, <code>right</code>, <code>full</code>, <code>left_semi</code>, <code>left_anti</code></p>"},{"location":"pyspark_interview_questions/#11-what-is-the-role-of-sparksession","title":"11) What is the role of SparkSession?","text":"<p>Definition: SparkSession is the single entry point to programming Spark in PySpark.</p> <pre><code>spark = SparkSession.builder.appName(\"app\").getOrCreate()\n</code></pre> <p>It replaces older objects like SQLContext and HiveContext.</p>"},{"location":"pyspark_interview_questions/#12-how-do-you-optimize-pyspark-jobs","title":"12) How do you optimize PySpark jobs?","text":"<p>Definition: Optimization means improving speed, reducing memory usage, and minimizing data shuffles.</p> <p>Key techniques: - Use Parquet instead of CSV (columnar + compressed). - Tune partitions: <code>df.repartition(200)</code>. - Avoid <code>collect()</code> on large data (prevents driver OOM errors). - Use broadcast joins for small tables. - Cache reused DataFrames: <code>df.cache()</code>. - Select only needed columns.</p>"},{"location":"pyspark_interview_questions/#13-differences-between-rdd-dataframe-dataset","title":"13) Differences between RDD, DataFrame, Dataset","text":"Structure Schema Performance Ease of use RDD No Lower Hard DataFrame Yes High Easy Dataset Yes High Medium"},{"location":"pyspark_interview_questions/#14-how-do-lazy-evaluation-and-dag-work-together","title":"14) How do lazy evaluation and DAG work together?","text":"<p>Definition: Spark builds a Directed Acyclic Graph (DAG) representing transformations before execution.</p> <p>Process: 1. User defines transformations. 2. Spark builds a logical DAG. 3. Optimizer refines the plan. 4. DAG is split into stages. 5. Tasks run in parallel on executors.</p>"},{"location":"pyspark_interview_questions/#15-more-performance-optimization-tips","title":"15) More performance optimization tips","text":"<ul> <li>Use Parquet + ZSTD compression.</li> <li>Avoid data skew.</li> <li>Use predicate pushdown.</li> <li>Increase executor memory if needed.</li> </ul>"},{"location":"pyspark_interview_questions/#16-how-do-you-handle-data-skew","title":"16) How do you handle data skew?","text":"<p>Definition: Data skew happens when some keys have far more records than others, causing slow tasks.</p> <p>Solutions: - Key salting - Broadcast small tables - Repartition on different keys - Use <code>skewHint</code> in Spark 3+</p>"},{"location":"pyspark_interview_questions/#17-what-are-broadcast-variables","title":"17) What are broadcast variables?","text":"<p>Definition: Broadcast variables efficiently share a small, read-only dataset with all executors.</p> <p>Why use them: - Reduce network shuffle - Speed up joins with small tables</p> <pre><code>from pyspark.sql.functions import broadcast\nfact_df.join(broadcast(dim_df), \"customer_id\")\n</code></pre>"},{"location":"pyspark_interview_questions/#18-custom-udf-in-pyspark","title":"18) Custom UDF in PySpark","text":"<p>Definition: A UDF allows you to apply custom Python logic to Spark DataFrames.</p> <pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\ndef square(x): return x*x\nsquare_udf = udf(square, IntegerType())\ndf = df.withColumn(\"squared\", square_udf(df[\"value\"]))\n</code></pre> <p>Note: Prefer built-in Spark functions when possible.</p>"},{"location":"pyspark_interview_questions/#19-how-do-you-monitor-and-debug-pyspark","title":"19) How do you monitor and debug PySpark?","text":"<p>Definition: Monitoring helps detect slow jobs, skew, memory issues, and failures.</p> <p>Tools: - Spark UI: http://localhost:4040 - AWS Glue CloudWatch logs - Databricks UI - YARN Resource Manager - <code>spark.sparkContext.setLogLevel(\"INFO\")</code></p>"},{"location":"pyspark_interview_questions/#add-this-to-mkdocs","title":"Add this to MkDocs","text":"<pre><code>nav:\n  - Home: index.md\n  - Python Interviews: python-interview.md\n  - Projects: projects.md\n  - PySpark Q&amp;A: pyspark-interview.md\n</code></pre>"},{"location":"python_interview/","title":"Python Interview Questions for Data Engineers","text":""},{"location":"python_interview/#basics","title":"\ud83e\udde0 Basics","text":"<p>Data Types in Python: Python offers numerous built-in data types that provide varying functionalities and utilities.</p>"},{"location":"python_interview/#immutable-data-types","title":"Immutable Data Types","text":"<ol> <li>int \u2013 Whole numbers like <code>42</code></li> <li>float \u2013 Decimal numbers like <code>3.14</code></li> <li>complex \u2013 Real + imaginary parts, e.g., <code>3+4j</code></li> <li>bool \u2013 <code>True</code> or <code>False</code></li> <li>str \u2013 Unicode characters</li> <li>tuple \u2013 Ordered, immutable collection</li> <li>frozenset \u2013 Immutable set</li> <li>bytes \u2013 Immutable byte sequence</li> <li>bytearray \u2013 Mutable bytes</li> <li>NoneType \u2013 Represents no value</li> </ol>"},{"location":"python_interview/#mutable-data-types","title":"Mutable Data Types","text":"<ol> <li>list \u2013 Ordered, dynamic collection</li> <li>set \u2013 Unique, unordered values</li> <li>dict \u2013 Key\u2013value pairs</li> <li>memoryview \u2013 View memory buffers</li> <li>array \u2013 Typed array</li> <li>deque \u2013 Double\u2011ended queue</li> <li>object \u2013 Root of all Python classes</li> <li>SimpleNamespace \u2013 Flexible attribute storage</li> <li>ModuleType \u2013 Module object</li> <li>FunctionType \u2013 Function object</li> </ol> <p>Q2. What\u2019s the difference between <code>is</code> and <code>==</code> in Python? - <code>==</code> compares values, <code>is</code> checks identity (memory location). is: Compares the memory address or identity of two objects. ==: Compares the content or value of two objects.</p> <p>Q4. What's the difference between a list and a tuple? - Lists are mutable; tuples are immutable and faster. Lists are ideal for collections that may change in size and content. They are the preferred choice for storing data elements.</p> <p>Tuples, due to their immutability and enhanced performance, are a good choice for representing fixed sets of related data.</p> <p>Q5. What's the difference between append and extend ? -  Append adds object to the end of list = [1,2,3,[4,5]] -  Extend adds individual objects to the list at the end</p> <p>What is the difference between Python Arrays and lists? Arrays in python can only contain elements of same data types i.e., data type of array should be homogeneous. It is a thin wrapper around C language arrays and consumes far less memory than lists. Lists in python can contain elements of different data types i.e., data type of lists can be heterogeneous. It has the disadvantage of consuming large memory.</p> <p>kwargs and args In Python, args and *kwargs are often used to pass a variable number of arguments to a function.</p> <p>args collects a variable number of positional arguments into a tuple, while *kwargs does the same for keyword arguments into a dictionary.</p> <p>The name args is a convention. The asterisk () tells Python to put any remaining positional arguments it receives into a tuple.</p> <p>The double asterisk (**) is used to capture keyword arguments and their values into a dictionary.</p> <p>args:  def sum_all(*args):     result = 0     for num in args:         result += num     return result</p> <p>print(sum_all(1, 2, 3, 4))  # Output: 10</p> <p>kwargs:  def print_values(**kwargs):     for key, value in kwargs.items():         print(f\"{key}: {value}\")</p> <p>Keyword arguments are captured as a dictionary print_values(name=\"John\", age=30, city=\"New York\")</p>"},{"location":"python_interview/#advanced-topics-for-data-engineering","title":"\ud83d\udee0\ufe0f Advanced Topics for Data Engineering","text":"<p>Q11. How to read a file line by line?</p> <pre><code>with open('file.txt') as f:\n    for line in f:\n        print(line)\n</code></pre> <p>Q21. How do you read a large file without loading into memory?</p> <pre><code>with open('large.csv') as f:\n    for line in f:\n        process(line)\n</code></pre>"},{"location":"tellmeaboutyourself/","title":"Tell me about yourself","text":"<p>I have been a data engineer for the past 8 years.</p> <p>My work has been majorly focussed on building data pipelines from diff data sources and loading them into aws redshift. I have worked on a lot of ETL using Airflow and Talend and informatica. Managing pipelines debugging them and fixing them. I have worked with python, sql and pyspark for transformations </p> <p>I have also worked on azure for sometime basically on adf to build pipelines and currently am working with talend, aws services like steo fun, redshift, glue and all that. </p> <p>====</p> <p>step functions bring source data do metadata transformations and upload to athena </p>"},{"location":"data_modeling/fact_dimension_tables/","title":"Dimension and Fact Tables","text":""},{"location":"data_modeling/fact_dimension_tables/#1-fact-tables","title":"1\ufe0f\u20e3 Fact Tables","text":"<p>Contain measurable, quantitative data.</p> Attribute Description Grain Level of detail (e.g., one row per sale) Foreign Keys Links to dimension tables Measures Numeric values (e.g., sales_amount, units_sold) <p>Example: | sale_id | date_key | product_key | customer_key | sales_amount | |----------|-----------|-------------|---------------|---------------| | 1 | 20231001 | 11 | 501 | 250.00 |</p>"},{"location":"data_modeling/fact_dimension_tables/#2-dimension-tables","title":"2\ufe0f\u20e3 Dimension Tables","text":"<p>Contain descriptive attributes related to facts.</p> Attribute Description Primary Key Unique identifier (used in fact tables) Attributes Non-measurable, descriptive info <p>Example: | customer_key | customer_name | region | |---------------|---------------|---------| | 501 | Alice | West Coast |</p>"},{"location":"data_modeling/fact_dimension_tables/#3-relationship","title":"3\ufe0f\u20e3 Relationship","text":"<p>Fact tables reference multiple dimension tables via foreign keys. Dimensions provide context, facts provide metrics.</p>"},{"location":"data_modeling/fact_dimension_tables/#4-summary","title":"4\ufe0f\u20e3 Summary","text":"Type Data Type Example Size Fact Table Quantitative Sales, Revenue Large Dimension Table Qualitative Customer, Product Small"},{"location":"data_modeling/oltp_vs_olap/","title":"OLTP vs OLAP","text":"<p>OLTP(Online Transaction Processing): Purpose: To manage and process day-to-day transactions in real-time. </p> <p>Characteristics:  Optimized for high-volume, fast transactions involving small amounts of data per transaction (e.g., one record).  Focuses on the current state of the data.  Designed for frequent inserts, updates, and deletes.  Used by frontline workers and customers for operational tasks.  Examples: Banking transactions, e-commerce order processing, airline reservations. </p> <p>OLAP (Online Analytical Processing): Purpose: To enable complex analysis of large volumes of historical data. </p> <p>Characteristics: Optimized for complex, read-heavy queries that aggregate large amounts of data.  Focuses on historical and aggregated data to support business intelligence and decision-making.  Uses multidimensional data cubes to allow for \"slicing,\" \"dicing,\" and \"pivoting\" data.  Used by data analysts, data scientists, and business intelligence professionals.  Examples: Analyzing sales trends over several years, forecasting future sales, or identifying customer buying patterns. </p>"},{"location":"data_modeling/oltp_vs_olap/#overview","title":"Overview","text":"Feature OLTP (Online Transaction Processing) OLAP (Online Analytical Processing) Purpose Day-to-day operations Analytical insights and reporting Data Type Current, up-to-date data Historical, summarized data Operations Insert, Update, Delete Read, Aggregate, Analyze Users Clerks, Frontline staff Analysts, Decision-makers Schema Normalized (3NF) Denormalized (Star/Snowflake) Query Speed Fast for small reads/writes Optimized for large aggregations Examples Banking, e-commerce transactions Data warehouses, dashboards"},{"location":"data_modeling/scd_types/","title":"Slowly Changing Dimensions (SCDs)","text":""},{"location":"data_modeling/scd_types/#what-is-an-scd","title":"What is an SCD?","text":"<p>A Slowly Changing Dimension tracks how dimension data changes over time. For example, when a customer moves to a new city, do we overwrite it or keep history?</p>"},{"location":"data_modeling/scd_types/#types-of-scds","title":"Types of SCDs","text":""},{"location":"data_modeling/scd_types/#type-1-overwrite","title":"\ud83d\udfe9 Type 1 \u2013 Overwrite","text":"<p>Old data is replaced by new data.</p> customer_key name city 501 Alice Seattle <p>\u27a1 Alice moves to Denver \u2192 Updated record (no history)</p> customer_key name city 501 Alice Denver <p>\u2705 Simple \u274c No history</p>"},{"location":"data_modeling/scd_types/#type-2-add-new-record","title":"\ud83d\udfe6 Type 2 \u2013 Add New Record","text":"<p>Old record is retained; new record is added with a new surrogate key.</p> customer_key name city start_date end_date 501 Alice Seattle 2022-01-01 2023-04-01 502 Alice Denver 2023-04-02 NULL <p>\u2705 Keeps history \u274c Grows table size  </p>"},{"location":"data_modeling/scd_types/#type-3-add-new-column","title":"\ud83d\udfe8 Type 3 \u2013 Add New Column","text":"<p>Store both old and new values in same record.</p> customer_key name old_city current_city 501 Alice Seattle Denver <p>\u2705 Simple and compact \u274c Only one level of history  </p>"},{"location":"data_modeling/scd_types/#summary-table","title":"Summary Table","text":"Type Description History Preserved? Type 1 Overwrite old data \u274c Type 2 Add new record \u2705 Type 3 Add new column \u26a0\ufe0f Partial"},{"location":"data_modeling/scd_types/#which-type-to-use","title":"Which Type to Use?","text":"<ul> <li>Type 1 \u2192 When changes are unimportant (e.g., typo fix)</li> <li>Type 2 \u2192 When tracking history is critical (e.g., address, department)</li> <li>Type 3 \u2192 When limited history is acceptable (e.g., current vs previous)</li> </ul> <p>Why are SCDs important?:</p> <p>Historical tracking: They allow you to track changes over time, which helps with accurate reporting and analysis.  For example, you can analyze a store's sales by its region before and after a reorganization.</p> <p>Data integrity: They ensure that historical data remains accurate, preventing metrics from becoming unreliable as dimensions change.</p> <p>Business insights: By capturing historical context, businesses can connect performance changes to the specific attribute versions that were in effect at the time. </p>"},{"location":"data_modeling/snowflake_schema/","title":"Snowflake Schema","text":""},{"location":"data_modeling/snowflake_schema/#what-is-a-snowflake-schema","title":"What is a Snowflake Schema?","text":"<p>A Snowflake Schema is a more normalized version of the Star Schema. Here, dimension tables are further split into sub-dimensions.</p>"},{"location":"data_modeling/star_schema/","title":"Star Schema","text":""},{"location":"data_modeling/star_schema/#what-is-a-star-schema","title":"What is a Star Schema?","text":"<p>A star schema is a type of data modeling technique used in data warehousing to represent data in a structured and intuitive way. In a star schema, data is organized into a central fact table that contains the measures of interest, surrounded by dimension tables that describe the attributes of the measures.</p> <p>Fact Table: The fact table in a star schema contains the measures or metrics that are of interest to the user or organization. </p> <p>Dimension Table:     &gt; The dimension tables in a star schema contain the descriptive attributes of the measures in the fact table.     &gt; Each dimension table is joined to the fact table through a foreign key relationship. This allows users to query the data in the fact table using attributes from the dimension tables.</p> <p>This schema is widely used to develop or build a data warehouse and dimensional data marts.  It includes one or more fact tables indexing any number of dimensional tables.  The star schema is a necessary cause of the snowflake schema.  It is also efficient for handling basic queries. </p> <p>Denormalized structure: A star schema is denormalized, which means that redundancy is allowed in the schema design to improve query performance. This is because it is easier and faster to join a small number of tables than a large number of tables.</p> <p>It is designed for fast query performance. This is because the schema is denormalized and data is pre-aggregated, making queries faster and more efficient.</p>"},{"location":"databases/redshift_spectrum/","title":"Redshift Spectrum","text":"<p>Redshift spectrum external tables creation need a glue db. It doesnt have to be existing you can just add a new name to the create external schema query and it shoudl work.</p> <p>CREATE EXTERNAL SCHEMA spectrum_ehr_claims_2026_02_03 FROM DATA CATALOG DATABASE 'ehr_claims_db'  IAM_ROLE 'arn:aws:iam::000000043252:role/redshiftrole'  </p> <p>CREATE EXTERNAL TABLE \"spectrum_ehr_claims_2026_02_03\".\"Condition\" (     \"AbatementDateTime\" TIMESTAMP,     \"BodySiteMapId\" BIGINT,     \"CategoryConceptId\" INT,     \"ClinicalStatusConceptId\" INT,     \"CodeConceptMapId\" BIGINT,     \"EncounterId\" VARCHAR(65535),     \"Id\" VARCHAR(65535),     \"OnsetDateTime\" TIMESTAMP,     \"PatientId\" VARCHAR(65535),     \"PersonId\" VARCHAR(65535),     \"PrimaryDiagnosisConceptId\" INT,     \"RecordedDateTime\" TIMESTAMP,     \"SequenceNumber\" SMALLINT,     \"SeverityConceptId\" INT,     \"SourceProvenanceConceptId\" INT,     \"SourceRecordId\" VARCHAR(65535),     \"VerificationStatusConceptId\" INT ) STORED AS PARQUET LOCATION 's3://bucketname/dataloadfolder/Condition/' TABLE PROPERTIES ('parquet.compress'='SNAPPY');</p>"},{"location":"sql/sql/","title":"SQL Interview Questions for Data Engineers","text":""},{"location":"sql/sql/#basic-queries","title":"\ud83e\uddfe Basic Queries","text":"<p>Q1. What is the difference between <code>WHERE</code> and <code>HAVING</code>? - <code>WHERE</code> filters rows before grouping; <code>HAVING</code> filters groups after aggregation.</p> <p>Q2. How do you fetch only unique values from a column? </p> <pre><code>SELECT DISTINCT column_name FROM table_name;\n</code></pre> <p>Q3. How to get the second highest salary from an employee table? </p> <pre><code>SELECT MAX(salary) FROM employees\nWHERE salary &lt; (SELECT MAX(salary) FROM employees);\n</code></pre> <p>Q4. How do you rename a column in the result set? </p> <pre><code>SELECT column_name AS new_name FROM table_name;\n</code></pre>"},{"location":"sql/sql/#aggregations-grouping","title":"\ud83e\uddee Aggregations &amp; Grouping","text":"<p>Q5. How to count the number of employees per department? </p> <pre><code>SELECT department, COUNT(*) FROM employees GROUP BY department;\n</code></pre> <p>Q6. How to calculate average salary by job role? </p> <pre><code>SELECT job_role, AVG(salary) FROM employees GROUP BY job_role;\n</code></pre> <p>Q7. How to filter groups with more than 10 employees? </p> <pre><code>SELECT department, COUNT(*) FROM employees\nGROUP BY department\nHAVING COUNT(*) &gt; 10;\n</code></pre>"},{"location":"sql/sql/#joins","title":"\ud83d\udd17 Joins","text":"<p>Q8. What\u2019s the difference between INNER JOIN and LEFT JOIN? - <code>INNER JOIN</code> returns matching rows; <code>LEFT JOIN</code> returns all from left + matches.</p> <p>Q9. How to get employees and their department names? </p> <pre><code>SELECT e.name, d.name\nFROM employees e\nJOIN departments d ON e.dept_id = d.id;\n</code></pre> <p>Q10. How to find employees without a manager (using LEFT JOIN)? </p> <pre><code>SELECT e.name FROM employees e\nLEFT JOIN employees m ON e.manager_id = m.id\nWHERE m.id IS NULL;\n</code></pre>"},{"location":"sql/sql/#subqueries-ctes","title":"\ud83d\udcd0 Subqueries &amp; CTEs","text":"<p>Q11. What is a CTE and when would you use it? - A Common Table Expression makes complex queries more readable and reusable.</p> <p>Q12. Write a query using a CTE to get top 5 salaries. </p> <pre><code>WITH ranked AS (\n  SELECT name, salary,\n         DENSE_RANK() OVER (ORDER BY salary DESC) AS rank\n  FROM employees\n)\nSELECT * FROM ranked WHERE rank &lt;= 5;\n</code></pre> <p>Q13. How to use a correlated subquery? </p> <pre><code>SELECT name FROM employees e\nWHERE salary &gt; (SELECT AVG(salary) FROM employees WHERE department = e.department);\n</code></pre>"},{"location":"sql/sql/#window-functions","title":"\ud83e\ude9f Window Functions","text":"<p>Q14. How to calculate running total of salaries? </p> <pre><code>SELECT name, salary,\n       SUM(salary) OVER (ORDER BY name) AS running_total\nFROM employees;\n</code></pre> <p>Q15. How to get each employee\u2019s rank based on salary within department? </p> <pre><code>SELECT name, department,\n       RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_rank\nFROM employees;\n</code></pre>"},{"location":"sql/sql/#miscellaneous","title":"\ud83d\udccb Miscellaneous","text":"<p>Q16. What\u2019s the difference between <code>UNION</code> and <code>UNION ALL</code>? - <code>UNION</code> removes duplicates, <code>UNION ALL</code> includes all.</p> <p>Q17. How to get rows between two dates? </p> <pre><code>SELECT * FROM orders\nWHERE order_date BETWEEN '2023-01-01' AND '2023-12-31';\n</code></pre> <p>Q18. How to delete duplicate records but keep one? </p> <pre><code>DELETE FROM employees\nWHERE id NOT IN (\n  SELECT MIN(id)\n  FROM employees\n  GROUP BY name, department\n);\n</code></pre> <p>Q19. What are indexes and why are they used? - Indexes speed up read queries by allowing faster lookup but slow down inserts/updates.</p> <p>Q20. How to find null values in a column? </p> <pre><code>SELECT * FROM employees WHERE manager_id IS NULL;\n</code></pre>"},{"location":"sql/sql_theory/","title":"SQL Theory","text":"<p>Primary Key: Unique column in a table No null values are allowed a table can have only 1 primary key</p> <p>diff between delete, drop, truncate: \u201cDELETE removes rows from a table and can be used with or without a WHERE clause. It is a DML operation and can be rolled back before commit.</p> <p>TRUNCATE removes all data from the table but keeps the table structure. It is faster than DELETE, does not support WHERE clause, and cannot be rolled back.</p> <p>DROP removes the entire table including its structure and data, and it cannot be rolled back.\u201d</p> <p>How would you find and remove duplicate records from a table in SQL? \u201cI use ROW_NUMBER with PARTITION BY to identify duplicates and remove rows where row_number is greater than 1.\u201d</p> <p>the below will help ony; to select dupliactes not delete. SELECT customer_id, order_date, COUNT() FROM orders GROUP BY customer_id, order_date HAVING COUNT() &gt; 1;</p> <p>\u201cIn ETL pipelines, I usually remove duplicates using SQL before loading or by using a Sorter + Aggregator transformation.\u201d</p> <p>What is the difference between WHERE and HAVING clauses? \u201cWHERE clause is used to filter rows before aggregation, whereas HAVING clause is used to filter aggregated results after GROUP BY.\u201d</p> <p>WHERE \u2192 filters rows before aggregation</p> <p>HAVING \u2192 filters groups after aggregation</p> <p>What is a CTE and why do we use it? \u201cA CTE, or Common Table Expression, is a temporary named result set that you can reference within a larger query. It helps simplify complex queries, improves readability, and avoids repeating the same subquery multiple times. Essentially, it makes SQL easier to write and maintain.\u201d</p> <p>What is a window function, and when would you use it instead of GROUP BY? \u201cA window function performs calculations across a set of rows related to the current row, without collapsing the rows like GROUP BY does. It\u2019s useful when you need aggregates along with detailed row-level data, such as running totals, ranks, or moving averages.\u201d</p> <p>You have a table orders with millions of rows. You need to load only the new records daily into your data warehouse. \ud83d\udc49 How would you handle this using SQL? \u201cI would use incremental loading by identifying new or changed records using a timestamp column like last_updated or a unique ID. In SQL, I would filter source data where last_updated &gt; last ETL run date and load only those rows. This reduces the volume of data processed and improves ETL performance.\u201d</p> <p>scd types</p> <p>SCD Type 1 \u2013 Overwrites data (no history) SCD Type 2 \u2013 Maintains full history SCD Type 3 \u2013 Limited history (column-based)</p> <p>Type 0 \u2192 Do nothing but keeps original data so nothing changes </p> <p>Type 1 \u2192 Overwrite/Updates the existing record. Does NOT maintain historical data</p> <p>Type 2 \u2192 Add rows (keeps full history)</p> <p>Type 3 \u2192 Keeps current + previous value in separate columns(limited history)</p>"}]}