1. Given a 5gb file how will you read in python and load to a DB
2. How will you bring 1000 files from oracle and what are the steps you will take to load and build a data pipeline either in aws or azure
3. what kind of data quality checks have you worked on?
4. What transformations have you done ?
5. How do you remove duplicates in pyspark [what other methods other than drop.duplicates do you know ?]
6. How do you remove nulls in pyspark ?
7. if you get number as data type from s3 and athena doesnt have that data type how will you handle that data type mismacth ? 
8. How do you update a col with another value in pyspark ?
9. How do you read excel file in pyspark (pyspark.read.csv()
11. What do you do as a data engineer ?