
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../sql/">
      
      
        <link rel="next" href="../projects/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>PySpark - Data Engineering Interview Prep</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2a3383ac.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pyspark-interview-questions-answers-with-definitions" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Data Engineering Interview Prep" class="md-header__button md-logo" aria-label="Data Engineering Interview Prep" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Data Engineering Interview Prep
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PySpark
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Data Engineering Interview Prep" class="md-nav__button md-logo" aria-label="Data Engineering Interview Prep" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Data Engineering Interview Prep
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Python
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Python
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../python_interview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pandas_interview_questions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pandas Interview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../sql/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SQL
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    PySpark
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    PySpark
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-what-is-pyspark-and-how-does-it-differ-from-regular-spark" class="md-nav__link">
    <span class="md-ellipsis">
      1) What is PySpark and how does it differ from regular Spark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-how-do-you-set-up-a-pyspark-environment" class="md-nav__link">
    <span class="md-ellipsis">
      2) How do you set up a PySpark environment?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-explain-the-architecture-of-apache-spark" class="md-nav__link">
    <span class="md-ellipsis">
      3) Explain the architecture of Apache Spark.
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-what-are-rdds-resilient-distributed-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      4) What are RDDs (Resilient Distributed Datasets)?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-how-do-dataframes-and-datasets-differ-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      5) How do DataFrames and Datasets differ in PySpark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-how-do-you-read-and-write-data-using-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      6) How do you read and write data using PySpark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-what-are-common-transformations-and-actions" class="md-nav__link">
    <span class="md-ellipsis">
      7) What are common transformations and actions?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-how-do-you-handle-missing-or-null-values" class="md-nav__link">
    <span class="md-ellipsis">
      8) How do you handle missing or null values?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-explain-lazy-evaluation-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      9) Explain lazy evaluation in PySpark.
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-how-do-you-perform-joins-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      10) How do you perform joins in PySpark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-what-is-the-role-of-sparksession" class="md-nav__link">
    <span class="md-ellipsis">
      11) What is the role of SparkSession?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-how-do-you-optimize-pyspark-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      12) How do you optimize PySpark jobs?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-differences-between-rdd-dataframe-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      13) Differences between RDD, DataFrame, Dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-how-do-lazy-evaluation-and-dag-work-together" class="md-nav__link">
    <span class="md-ellipsis">
      14) How do lazy evaluation and DAG work together?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-more-performance-optimization-tips" class="md-nav__link">
    <span class="md-ellipsis">
      15) More performance optimization tips
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16-how-do-you-handle-data-skew" class="md-nav__link">
    <span class="md-ellipsis">
      16) How do you handle data skew?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#17-what-are-broadcast-variables" class="md-nav__link">
    <span class="md-ellipsis">
      17) What are broadcast variables?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#18-custom-udf-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      18) Custom UDF in PySpark
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#19-how-do-you-monitor-and-debug-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      19) How do you monitor and debug PySpark?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="19) How do you monitor and debug PySpark?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add-this-to-mkdocs" class="md-nav__link">
    <span class="md-ellipsis">
      Add this to MkDocs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projects
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../actual_interview_questions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Real World Interview Questions
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Data Modeling
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Data Modeling
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data_modeling/oltp_vs_olap/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OLTP vs OLAP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data_modeling/star_schema/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Star Schema
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data_modeling/snowflake_schema/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Snowflake Schema
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data_modeling/fact_dimension_tables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dimension & Fact Tables
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data_modeling/scd_types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Slowly Changing Dimensions (SCDs)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-what-is-pyspark-and-how-does-it-differ-from-regular-spark" class="md-nav__link">
    <span class="md-ellipsis">
      1) What is PySpark and how does it differ from regular Spark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-how-do-you-set-up-a-pyspark-environment" class="md-nav__link">
    <span class="md-ellipsis">
      2) How do you set up a PySpark environment?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-explain-the-architecture-of-apache-spark" class="md-nav__link">
    <span class="md-ellipsis">
      3) Explain the architecture of Apache Spark.
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-what-are-rdds-resilient-distributed-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      4) What are RDDs (Resilient Distributed Datasets)?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-how-do-dataframes-and-datasets-differ-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      5) How do DataFrames and Datasets differ in PySpark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-how-do-you-read-and-write-data-using-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      6) How do you read and write data using PySpark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-what-are-common-transformations-and-actions" class="md-nav__link">
    <span class="md-ellipsis">
      7) What are common transformations and actions?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-how-do-you-handle-missing-or-null-values" class="md-nav__link">
    <span class="md-ellipsis">
      8) How do you handle missing or null values?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-explain-lazy-evaluation-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      9) Explain lazy evaluation in PySpark.
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-how-do-you-perform-joins-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      10) How do you perform joins in PySpark?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-what-is-the-role-of-sparksession" class="md-nav__link">
    <span class="md-ellipsis">
      11) What is the role of SparkSession?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-how-do-you-optimize-pyspark-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      12) How do you optimize PySpark jobs?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-differences-between-rdd-dataframe-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      13) Differences between RDD, DataFrame, Dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-how-do-lazy-evaluation-and-dag-work-together" class="md-nav__link">
    <span class="md-ellipsis">
      14) How do lazy evaluation and DAG work together?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-more-performance-optimization-tips" class="md-nav__link">
    <span class="md-ellipsis">
      15) More performance optimization tips
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16-how-do-you-handle-data-skew" class="md-nav__link">
    <span class="md-ellipsis">
      16) How do you handle data skew?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#17-what-are-broadcast-variables" class="md-nav__link">
    <span class="md-ellipsis">
      17) What are broadcast variables?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#18-custom-udf-in-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      18) Custom UDF in PySpark
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#19-how-do-you-monitor-and-debug-pyspark" class="md-nav__link">
    <span class="md-ellipsis">
      19) How do you monitor and debug PySpark?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="19) How do you monitor and debug PySpark?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add-this-to-mkdocs" class="md-nav__link">
    <span class="md-ellipsis">
      Add this to MkDocs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="pyspark-interview-questions-answers-with-definitions">PySpark Interview Questions &amp; Answers (With Definitions)</h1>
<p>This page is formatted for <strong>MkDocs</strong> and can be saved as:</p>
<pre><code>docs/pyspark-interview.md
</code></pre>
<hr />
<h2 id="1-what-is-pyspark-and-how-does-it-differ-from-regular-spark">1) What is PySpark and how does it differ from regular Spark?</h2>
<p><strong>Definition:</strong><br />
Apache Spark is a distributed computing engine for large-scale data processing. <strong>PySpark</strong> is the Python API for Apache Spark that allows you to write Spark applications using Python while Spark executes them on a distributed JVM-based engine.</p>
<p><strong>How it differs from regular (Scala) Spark:</strong>
- Spark core is written in <strong>Scala/Java</strong> and runs on the JVM.
- PySpark provides a <strong>Python interface</strong>, but computations are still executed by the Spark engine.
- PySpark is easier for Python users, but may introduce slight serialization overhead compared to native Scala.
- Performance is generally comparable when using DataFrames (because of Catalyst optimization).</p>
<hr />
<h2 id="2-how-do-you-set-up-a-pyspark-environment">2) How do you set up a PySpark environment?</h2>
<p><strong>Definition:</strong><br />
A PySpark environment is a runtime setup where Python can communicate with a Spark cluster to execute distributed computations.</p>
<p><strong>Common ways to set it up:</strong></p>
<p><strong>Local machine:</strong></p>
<pre><code class="language-bash">pip install pyspark
</code></pre>
<pre><code class="language-python">from pyspark.sql import SparkSession
spark = SparkSession.builder.appName(&quot;test&quot;).getOrCreate()
</code></pre>
<p><strong>Cloud/Production options:</strong>
- <strong>AWS Glue</strong> – Managed Spark environment for ETL.
- <strong>Databricks</strong> – Managed notebooks + clusters.
- <strong>AWS EMR</strong> – You create and manage a Spark cluster.</p>
<hr />
<h2 id="3-explain-the-architecture-of-apache-spark">3) Explain the architecture of Apache Spark.</h2>
<p><strong>Definition:</strong><br />
Spark uses a distributed master–worker architecture to process data in parallel across a cluster.</p>
<p><strong>Components:</strong>
- <strong>Driver:</strong> Controls the application, creates the DAG, and schedules tasks.
- <strong>Cluster Manager:</strong> (YARN, Mesos, or Standalone) allocates resources.
- <strong>Executors:</strong> Run tasks on worker nodes and store cached data.
- <strong>Tasks:</strong> Smallest unit of work executed on partitions.</p>
<p><strong>Execution flow:</strong></p>
<pre><code>Driver → Logical Plan → Optimized Plan → DAG → Stages → Tasks → Executors
</code></pre>
<hr />
<h2 id="4-what-are-rdds-resilient-distributed-datasets">4) What are RDDs (Resilient Distributed Datasets)?</h2>
<p><strong>Definition:</strong><br />
An RDD is Spark’s original distributed data structure representing an immutable, partitioned collection of elements that can be processed in parallel.</p>
<p><strong>Key properties:</strong>
- Distributed across nodes
- Immutable
- Fault-tolerant via lineage
- Lazy evaluated
- Partitioned for parallelism</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">rdd = spark.sparkContext.parallelize([1,2,3,4])
</code></pre>
<hr />
<h2 id="5-how-do-dataframes-and-datasets-differ-in-pyspark">5) How do DataFrames and Datasets differ in PySpark?</h2>
<p><strong>Definition:</strong>
- A <strong>DataFrame</strong> is a distributed table with named columns and a schema.
- A <strong>Dataset</strong> is a strongly typed distributed collection (mainly used in Scala/Java).</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>DataFrame</th>
<th>Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td>Schema</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Type safety</td>
<td>No (Python)</td>
<td>Yes (Scala/Java)</td>
</tr>
<tr>
<td>Performance</td>
<td>Optimized</td>
<td>Optimized</td>
</tr>
<tr>
<td>Common in PySpark</td>
<td>Yes</td>
<td>Rare</td>
</tr>
</tbody>
</table>
<p><strong>In practice:</strong> Most PySpark work uses <strong>DataFrames</strong>.</p>
<hr />
<h2 id="6-how-do-you-read-and-write-data-using-pyspark">6) How do you read and write data using PySpark?</h2>
<p><strong>Definition:</strong><br />
PySpark provides a unified data source API to read/write structured data from files, databases, and cloud storage.</p>
<p><strong>Read CSV:</strong></p>
<pre><code class="language-python">df = spark.read.csv(&quot;s3://bucket/file.csv&quot;, header=True, inferSchema=True)
</code></pre>
<p><strong>Write Parquet:</strong></p>
<pre><code class="language-python">df.write.mode(&quot;overwrite&quot;).parquet(&quot;s3://bucket/output/&quot;)
</code></pre>
<p><strong>Read Parquet:</strong></p>
<pre><code class="language-python">df = spark.read.parquet(&quot;s3://bucket/output/&quot;)
</code></pre>
<hr />
<h2 id="7-what-are-common-transformations-and-actions">7) What are common transformations and actions?</h2>
<p><strong>Definition:</strong><br />
- <strong>Transformations</strong> create a new DataFrame/RDD but do not execute immediately (lazy).
- <strong>Actions</strong> trigger execution of the Spark job.</p>
<p><strong>Common transformations:</strong>
- <code>select()</code>, <code>filter()</code>, <code>withColumn()</code>, <code>groupBy()</code>, <code>join()</code></p>
<p><strong>Common actions:</strong>
- <code>show()</code>, <code>count()</code>, <code>collect()</code>, <code>write()</code></p>
<hr />
<h2 id="8-how-do-you-handle-missing-or-null-values">8) How do you handle missing or null values?</h2>
<p><strong>Definition:</strong><br />
Missing values (nulls) must be cleaned before analytics to avoid incorrect results.</p>
<pre><code class="language-python">df.dropna()
df.fillna(0)
df.fillna({&quot;age&quot;: 30, &quot;salary&quot;: 0})
</code></pre>
<hr />
<h2 id="9-explain-lazy-evaluation-in-pyspark">9) Explain lazy evaluation in PySpark.</h2>
<p><strong>Definition:</strong><br />
Lazy evaluation means Spark <strong>does not execute transformations immediately</strong>. Instead, it builds a logical plan (DAG) and waits for an action before running.</p>
<p><strong>Why this matters:</strong>
- Allows Spark to optimize the full pipeline.
- Reduces unnecessary computation.</p>
<hr />
<h2 id="10-how-do-you-perform-joins-in-pyspark">10) How do you perform joins in PySpark?</h2>
<pre><code class="language-python">df1.join(df2, on=&quot;id&quot;, how=&quot;inner&quot;)
</code></pre>
<p><strong>Join types:</strong>
- <code>inner</code>, <code>left</code>, <code>right</code>, <code>full</code>, <code>left_semi</code>, <code>left_anti</code></p>
<hr />
<h2 id="11-what-is-the-role-of-sparksession">11) What is the role of SparkSession?</h2>
<p><strong>Definition:</strong><br />
SparkSession is the single entry point to programming Spark in PySpark.</p>
<pre><code class="language-python">spark = SparkSession.builder.appName(&quot;app&quot;).getOrCreate()
</code></pre>
<p>It replaces older objects like SQLContext and HiveContext.</p>
<hr />
<h2 id="12-how-do-you-optimize-pyspark-jobs">12) How do you optimize PySpark jobs?</h2>
<p><strong>Definition:</strong><br />
Optimization means improving speed, reducing memory usage, and minimizing data shuffles.</p>
<p>Key techniques:
- Use <strong>Parquet instead of CSV</strong> (columnar + compressed).
- Tune partitions: <code>df.repartition(200)</code>.
- Avoid <code>collect()</code> on large data (prevents driver OOM errors).
- Use <strong>broadcast joins</strong> for small tables.
- Cache reused DataFrames: <code>df.cache()</code>.
- Select only needed columns.</p>
<hr />
<h2 id="13-differences-between-rdd-dataframe-dataset">13) Differences between RDD, DataFrame, Dataset</h2>
<table>
<thead>
<tr>
<th>Structure</th>
<th>Schema</th>
<th>Performance</th>
<th>Ease of use</th>
</tr>
</thead>
<tbody>
<tr>
<td>RDD</td>
<td>No</td>
<td>Lower</td>
<td>Hard</td>
</tr>
<tr>
<td>DataFrame</td>
<td>Yes</td>
<td>High</td>
<td>Easy</td>
</tr>
<tr>
<td>Dataset</td>
<td>Yes</td>
<td>High</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="14-how-do-lazy-evaluation-and-dag-work-together">14) How do lazy evaluation and DAG work together?</h2>
<p><strong>Definition:</strong><br />
Spark builds a <strong>Directed Acyclic Graph (DAG)</strong> representing transformations before execution.</p>
<p>Process:
1. User defines transformations.
2. Spark builds a logical DAG.
3. Optimizer refines the plan.
4. DAG is split into stages.
5. Tasks run in parallel on executors.</p>
<hr />
<h2 id="15-more-performance-optimization-tips">15) More performance optimization tips</h2>
<ul>
<li>Use Parquet + ZSTD compression.</li>
<li>Avoid data skew.</li>
<li>Use predicate pushdown.</li>
<li>Increase executor memory if needed.</li>
</ul>
<hr />
<h2 id="16-how-do-you-handle-data-skew">16) How do you handle data skew?</h2>
<p><strong>Definition:</strong><br />
Data skew happens when some keys have far more records than others, causing slow tasks.</p>
<p>Solutions:
- Key salting
- Broadcast small tables
- Repartition on different keys
- Use <code>skewHint</code> in Spark 3+</p>
<hr />
<h2 id="17-what-are-broadcast-variables">17) What are broadcast variables?</h2>
<p><strong>Definition:</strong><br />
Broadcast variables efficiently share a small, read-only dataset with all executors.</p>
<p><strong>Why use them:</strong>
- Reduce network shuffle
- Speed up joins with small tables</p>
<pre><code class="language-python">from pyspark.sql.functions import broadcast
fact_df.join(broadcast(dim_df), &quot;customer_id&quot;)
</code></pre>
<hr />
<h2 id="18-custom-udf-in-pyspark">18) Custom UDF in PySpark</h2>
<p><strong>Definition:</strong><br />
A UDF allows you to apply custom Python logic to Spark DataFrames.</p>
<pre><code class="language-python">from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

def square(x): return x*x
square_udf = udf(square, IntegerType())
df = df.withColumn(&quot;squared&quot;, square_udf(df[&quot;value&quot;]))
</code></pre>
<p><strong>Note:</strong> Prefer built-in Spark functions when possible.</p>
<hr />
<h2 id="19-how-do-you-monitor-and-debug-pyspark">19) How do you monitor and debug PySpark?</h2>
<p><strong>Definition:</strong><br />
Monitoring helps detect slow jobs, skew, memory issues, and failures.</p>
<p>Tools:
- Spark UI: http://localhost:4040
- AWS Glue CloudWatch logs
- Databricks UI
- YARN Resource Manager
- <code>spark.sparkContext.setLogLevel("INFO")</code></p>
<hr />
<h3 id="add-this-to-mkdocs">Add this to MkDocs</h3>
<pre><code class="language-yaml">nav:
  - Home: index.md
  - Python Interviews: python-interview.md
  - Projects: projects.md
  - PySpark Q&amp;A: pyspark-interview.md
</code></pre>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>